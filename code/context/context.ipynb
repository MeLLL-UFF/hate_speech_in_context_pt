{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import umap\n",
    "from umap import UMAP\n",
    "import string\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = 'dataset_path/'\n",
    "data = '' #hatebr, toldbr, olidbr\n",
    "train_file = src_path + f'''/{data}_train_balanced.csv'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_file)\n",
    "\n",
    "classe = 0 #0 -- neutro, 1 -- ofensivo, 2 -- discurso de odio\n",
    "train_data = train_data[train_data['label'] == classe]\n",
    "\n",
    "total_text = train_data[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Portuguese model\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def remove_verbs_propernouns_possessivepronouns(texto):\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # Remove hyperlinks\n",
    "    texto = re.sub(r'http\\S+', '', texto)\n",
    "\n",
    "    # Process text with spaCy to get tokens\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    # Possessive pronouns list\n",
    "    possessive_pronouns = ['meu', 'minha', 'meus', 'minhas', 'teu', 'tua', 'teus', 'tuas', 'seu', 'sua', 'seus', 'suas',\n",
    "                           'nosso', 'nossa', 'nossos', 'nossas', 'vosso', 'vossa', 'vossos', 'vossas', 'desse', 'desses',\n",
    "                           'dessa', 'dessas', 'aquele', 'aquela', 'aqueles', 'aquelas', 'pra','de', 'em', 'nao']\n",
    "\n",
    "    # List of common stopwords\n",
    "    stop_words_spacy = set(spacy.lang.pt.stop_words.STOP_WORDS)\n",
    "    stop_words_nltk = set(stopwords.words('portuguese'))\n",
    "    stop_words = stop_words_spacy.union(stop_words_nltk)\n",
    "    stop_words.add('httpurl')\n",
    "\n",
    "    # A helper function to clean a single token\n",
    "    def clean_token(token):\n",
    "        if re.match(r':[\\w_]+:', token.text):  # Check for emojis\n",
    "            return token.text\n",
    "        else:\n",
    "            cleaned = ''.join([char for char in token.lemma_ if char not in string.punctuation])  # Using token.lemma_ for lemmatization\n",
    "            return cleaned if cleaned else ''\n",
    "\n",
    "    # Apply logic\n",
    "    tokens_cleaned = [\n",
    "        clean_token(token)\n",
    "        for token in doc\n",
    "        if (token.text.lower() not in stop_words\n",
    "        and token.pos_ != \"VERB\"\n",
    "        and token.pos_ != \"PROPN\"\n",
    "        and token.text.lower() not in possessive_pronouns)\n",
    "        or re.match(r':[\\w_]+:', token.text)  # Keep emojis in the format :word:\n",
    "    ]\n",
    "\n",
    "    tokens_cleaned = [token for token in tokens_cleaned if len(token) > 2 and token != '']\n",
    "\n",
    "    # Join the tokens back together\n",
    "    cleaned_text = ' '.join(tokens_cleaned).strip()\n",
    "\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    # If the cleaned text is empty after removing, return None (so you can drop these rows later)\n",
    "    return cleaned_text if cleaned_text else None\n",
    "\n",
    "# Example of usage\n",
    "clean_text = [remove_verbs_propernouns_possessivepronouns(text) for text in total_text]\n",
    "\n",
    "total_text_df = pd.DataFrame({\n",
    "    'text': total_text,\n",
    "    'clean_text': clean_text\n",
    "})\n",
    "\n",
    "total_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out None, empty string values, and very short strings\n",
    "filtered_texts = total_text_df['clean_text'].dropna()\n",
    "filtered_texts = filtered_texts[filtered_texts.str.strip() != '']\n",
    "filtered_texts = filtered_texts[filtered_texts.str.len() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_words = [word for text in filtered_texts.dropna() for word in str(text).split() if len(word) < 3]\n",
    "short_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all words in the clean_text column\n",
    "all_words = [word for text in total_text_df['clean_text'].dropna() for word in str(text).split()]\n",
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out None values from clean_text\n",
    "filtered_clean_text = [text for text in clean_text if text is not None]\n",
    "\n",
    "# Contar a frequência de cada palavra\n",
    "word_freq = Counter(\" \".join(filtered_clean_text).split())\n",
    "\n",
    "# Selecionar as N palavras mais frequentes\n",
    "N = 20\n",
    "most_common_words = word_freq.most_common(N)\n",
    "\n",
    "# Preparar os dados para o gráfico\n",
    "words, frequencies = zip(*most_common_words)\n",
    "\n",
    "# Plotar o gráfico de barras\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.barh(words, frequencies, color='skyblue')\n",
    "plt.xlabel('Frequência')\n",
    "plt.ylabel('Palavras')\n",
    "plt.title(f'As {N} Palavras Mais Frequentes')\n",
    "plt.gca().invert_yaxis()  # Inverter o eixo y para a palavra mais frequente aparecer no topo\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Count all the words in the clean_text column\n",
    "all_words = \" \".join(total_text_df['clean_text'].dropna()).split()\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Convert the counter object to a DataFrame\n",
    "word_df = pd.DataFrame(word_counts.items(), columns=['Palavra', 'Quantidade'])\n",
    "\n",
    "# Sort the DataFrame by the 'Quantidade' column in descending order and take the top 20\n",
    "word_df = word_df.sort_values(by='Quantidade', ascending=False).head(20).reset_index(drop=True)\n",
    "\n",
    "# Display the top 20 words\n",
    "word_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"melll-uff/bertweetbr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(language='portuguese', embedding_model=model,  umap_model=umap_model, calculate_probabilities=True)\n",
    "topics_total, probabilities = topic_model.fit_transform(filtered_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info()\n",
    "\n",
    "topic = pd.DataFrame()\n",
    "\n",
    "topics = []\n",
    "for i in range(0, len(topic_model.get_topic_info())):\n",
    "  top = []\n",
    "  name = 'Topic '+ str(i + 1)\n",
    "  topic_nr = freq.iloc[i][\"Topic\"]\n",
    "  for j in range(len(topic_model.get_topic(topic_nr))):\n",
    "    top.append(topic_model.get_topic(topic_nr)[j][0])\n",
    "  topics.append(top)\n",
    "  topic[name] = top\n",
    "topic"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
