{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = 'dataset_path/'\n",
    "data = '' #hatebr, toldbr, olidbr\n",
    "train_file = src_path + f'''/{data}_train_balanced.csv'''\n",
    "test_file = src_path + f'''/{data}_test.csv'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "classe = 0 #0 -- neutro, 1 -- ofensivo, 2 -- discurso de odio\n",
    "train_data = train_data[train_data['label'] == classe] \n",
    "\n",
    "\n",
    "train_texts = train_data[\"text\"].tolist()\n",
    "test_texts = test_data[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('melll-uff/bertweetbr')\n",
    "model = AutoModel.from_pretrained('melll-uff/bertweetbr')\n",
    "num_pal = 9 #mudar em relacao ao numero de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = get_embeddings(train_data['text'])\n",
    "train_embeddings_norm = normalize(train_embeddings, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = get_embeddings(test_data['text'])\n",
    "test_embeddings_norm = normalize(test_embeddings, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 11):  \n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
    "    kmeans.fit(train_embeddings_norm)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Regra do Cotovelo')\n",
    "plt.xlabel('Número de clusters')\n",
    "plt.ylabel('WCSS')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wcss(train_embeddings_norm):\n",
    "    wcss = []\n",
    "    for n in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "        kmeans.fit(train_embeddings_norm)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    return wcss\n",
    "\n",
    "def find_elbow(wcss):\n",
    "    x1, y1 = 1, wcss[0]\n",
    "    x2, y2 = len(wcss), wcss[-1]\n",
    "\n",
    "    distances = []\n",
    "    for i in range(len(wcss)):\n",
    "        x0 = i + 1\n",
    "        y0 = wcss[i]\n",
    "        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)\n",
    "        denominator = np.sqrt((y2 - y1)**2 + (x2 - x1)**2)\n",
    "        distances.append(numerator / denominator)\n",
    "\n",
    "    return distances.index(max(distances)) + 1\n",
    "\n",
    "wcss = calculate_wcss(train_embeddings_norm)\n",
    "elbow_point = find_elbow(wcss)\n",
    "\n",
    "print(f'O ponto de cotovelo é {elbow_point} clusters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_scores = []\n",
    "for i in range(2, 11):  # o número mínimo de clusters deve ser 2\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(train_embeddings_norm)\n",
    "    sil_score = silhouette_score(train_embeddings_norm, kmeans.labels_)\n",
    "    sil_scores.append(sil_score)\n",
    "\n",
    "plt.plot(range(2, 11), sil_scores)\n",
    "plt.title('Silhouette Score')\n",
    "plt.xlabel('Número de clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = elbow_point\n",
    "\n",
    "kmeans_train = KMeans(n_clusters=n_clusters, random_state=0).fit(train_embeddings_norm)\n",
    "centroids_train = kmeans_train.cluster_centers_\n",
    "train_labels = kmeans_train.fit_predict(train_embeddings_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_and_farthest_clusters(train_embeddings, train_labels, test_embeddings, n_clusters):\n",
    "    cluster_distances = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_indices = np.where(train_labels == i)[0]\n",
    "        cluster_embeddings = train_embeddings[cluster_indices]\n",
    "\n",
    "        # Calculando a distância média do cluster aos textos de teste\n",
    "        distances_to_test = [1 - cosine_similarity([cluster_center], [test_emb])[0][0]\n",
    "                             for test_emb in test_embeddings for cluster_center in cluster_embeddings]\n",
    "        cluster_avg_distance = np.mean(distances_to_test)\n",
    "        cluster_distances.append((i, cluster_avg_distance))\n",
    "\n",
    "    # Ordenando os clusters pela distância média\n",
    "    cluster_distances.sort(key=lambda x: x[1])\n",
    "\n",
    "    nearest_cluster, _ = cluster_distances[0]\n",
    "    farthest_cluster, _ = cluster_distances[-1]\n",
    "\n",
    "    nearest_cluster_indices = np.where(train_labels == nearest_cluster)[0]\n",
    "    farthest_cluster_indices = np.where(train_labels == farthest_cluster)[0]\n",
    "\n",
    "    return nearest_cluster_indices, farthest_cluster_indices\n",
    "\n",
    "nearest_indices, farthest_indices = find_nearest_and_farthest_clusters(train_embeddings_norm, train_labels, test_embeddings_norm, n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_indices, farthest_indices = find_nearest_and_farthest_clusters(train_embeddings_norm, train_labels, test_embeddings_norm, n_clusters)\n",
    "\n",
    "nearest_texts = train_data.iloc[nearest_indices]['text'] \n",
    "farthest_texts = train_data.iloc[farthest_indices]['text']\n",
    "\n",
    "print(\"Textos mais próximos:\")\n",
    "print(nearest_texts.head())\n",
    "print(\"\\nTextos mais distantes:\")\n",
    "print(farthest_texts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_distance_to_test(cluster_indices, test_embeddings, train_embeddings):\n",
    "    distances = [1 - cosine_similarity([train_emb], [test_emb])[0][0]\n",
    "                 for train_emb in train_embeddings[cluster_indices]\n",
    "                 for test_emb in test_embeddings]\n",
    "    return np.mean(distances)\n",
    "\n",
    "def get_texts_sorted_by_distance(cluster_indices, train_data, test_embeddings, train_embeddings, ascending=True, n=2):\n",
    "    distances = [calculate_average_distance_to_test([idx], test_embeddings, train_embeddings)\n",
    "                 for idx in cluster_indices]\n",
    "    sorted_indices = [x for _, x in sorted(zip(distances, cluster_indices), key=lambda pair: pair[0], reverse=not ascending)]\n",
    "    sorted_texts = train_data.iloc[sorted_indices][:n]['text']  \n",
    "    return sorted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_texts_sorted = get_texts_sorted_by_distance(nearest_indices, train_data, test_embeddings_norm, train_embeddings_norm, ascending=True, n=2)\n",
    "farthest_texts_sorted = get_texts_sorted_by_distance(farthest_indices, train_data, test_embeddings_norm, train_embeddings_norm, ascending=False, n=2)\n",
    "\n",
    "# Visualizando os textos ordenados\n",
    "print(\"Textos mais próximos, ordenados por proximidade:\")\n",
    "print(nearest_texts_sorted.head())\n",
    "print(\"\\nTextos mais distantes, ordenados por proximidade:\")\n",
    "print(farthest_texts_sorted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tamanho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_stopword_count(text, stopwords):\n",
    "    words = text.split()\n",
    "    non_stopword_count = sum(1 for word in words if word.lower() not in stopwords)\n",
    "    return non_stopword_count\n",
    "\n",
    "def word_count(text):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "def get_texts_sorted_by_distance_filtered(cluster_indices, train_data, test_embeddings, train_embeddings, ascending=True, n=2):\n",
    "    distances = [calculate_average_distance_to_test([idx], test_embeddings, train_embeddings)\n",
    "                 for idx in cluster_indices]\n",
    "    sorted_indices = [x for _, x in sorted(zip(distances, cluster_indices), key=lambda pair: pair[0], reverse=not ascending)]\n",
    "\n",
    "    filtered_indices = [idx for idx in sorted_indices if word_count(train_data.iloc[idx]['text']) == num_pal]\n",
    "\n",
    "    sorted_texts = train_data.iloc[filtered_indices][:n]['text']\n",
    "\n",
    "    return sorted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(text, tokenizer):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def get_texts_sorted_by_distance_filtered(cluster_indices, train_data, test_embeddings, train_embeddings, tokenizer, ascending=True, n=2, num_tokens=50):\n",
    "    distances = [calculate_average_distance_to_test([idx], test_embeddings, train_embeddings)\n",
    "                 for idx in cluster_indices]\n",
    "    sorted_indices = [x for _, x in sorted(zip(distances, cluster_indices), key=lambda pair: pair[0], reverse=not ascending)]\n",
    "\n",
    "    filtered_indices = [idx for idx in sorted_indices if token_count(train_data.iloc[idx]['text'], tokenizer) == num_tokens]\n",
    "\n",
    "    sorted_texts = train_data.iloc[filtered_indices][:n]['text']\n",
    "\n",
    "    return sorted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_texts_sorted_filtered = get_texts_sorted_by_distance_filtered(nearest_indices, train_data, test_embeddings_norm, train_embeddings_norm, ascending=True, n=4)\n",
    "\n",
    "print(nearest_texts_sorted_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "farthest_texts_sorted_filtered = get_texts_sorted_by_distance_filtered(farthest_indices, train_data, test_embeddings_norm, train_embeddings_norm, ascending=False, n=4)\n",
    "\n",
    "print(farthest_texts_sorted_filtered.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
